{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# search parameters\n",
        "num_iters = 12\n",
        "num_variations = 10\n",
        "num_randomized = 3\n",
        "model_iters = 5\n",
        "download_depth_models = True\n",
        "num_images_per_variation = 32\n",
        "num_image_proc_threads = 4\n",
        "do_vision_test_mode = False\n",
        "do_load_tabby_model = True\n",
        "quadro_delay = 1.0\n",
        "delay_3090 = 0.5\n",
        "batchsize_3090 = 25\n",
        "\n",
        "total_num_variations_per_steps = num_variations * (num_randomized + 1) * model_iters\n",
        "total_images_per_step = total_num_variations_per_steps * num_images_per_variation\n",
        "\n",
        "model_iter = 4\n",
        "\n",
        "dpo_lora_id = \"llama8B_vision_2_temp_DPO/checkpoint-500\"\n",
        "use_dpo_lora = False\n",
        "model_id = \"./llama8B_vision_4\"\n",
        "do_clear_screenshot_dir = True\n",
        "do_open_best_images = False\n",
        "\n",
        "use_multi_gpu_llm = False\n",
        "use_multi_gpu_vision = False\n",
        "\n",
        "maxModelFaces = 5 * 1000\n",
        "minModelFaces = 1 * 1000\n",
        "min_image_size = 350\n",
        "freeze_model_database_models = False\n",
        "\n",
        "model_download_path = \"./node_webgl_render_agentic/public/downloaded_models\"\n",
        "\n",
        "sketchfab_api_token = \"----\"\n",
        "openai_api_token = \"----\"\n",
        "huggingface_key = \"----\"\n",
        "\n",
        "small_ai_model = \"gpt-4o-mini\"\n",
        "normal_ai_model = \"gpt-4o\"\n",
        "\n",
        "max_sleeps = 400\n",
        "\n",
        "print(\"total_num_variations_per_steps\", total_num_variations_per_steps)\n",
        "print(\"total_images_per_step\", total_images_per_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import json\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "from io import BytesIO\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import time\n",
        "import open_clip\n",
        "from openai import OpenAI\n",
        "import zipfile\n",
        "from huggingface_hub import login\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "from peft import PeftModel\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Configuration\n",
        "# --------------------------------------------------------------------------\n",
        "TABBY_BASE_URLS = [ \"http://localhost:5000\", \"http://localhost:5001\" ]\n",
        "TABBY_ADMIN_KEY = \"----\"   # Must be admin-level token to load/unload models\n",
        "MODEL_NAME = \"llama8B_Vision_4\"    # The exact folder name in your TabbyAPI `models/` directory\n",
        "\n",
        "def is_model_loaded(url, model_name):\n",
        "    \"\"\"\n",
        "    Returns True if the model specified by `model_name` is currently loaded.\n",
        "    \"\"\"\n",
        "    url_model = f\"{url}/v1/model\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {TABBY_ADMIN_KEY}\"\n",
        "    }\n",
        "    response = requests.get(url_model, headers=headers)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        current_model = data.get(\"model_name\")\n",
        "        return current_model == model_name\n",
        "    else:\n",
        "        # Depending on how your API signals no model loaded, you may adjust this.\n",
        "        return False\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 1) Load Model\n",
        "# --------------------------------------------------------------------------\n",
        "def load_model(url, model_name, max_seq_len=4096 * 2):\n",
        "    if is_model_loaded(TABBY_BASE_URLS[0], MODEL_NAME):\n",
        "        return\n",
        "    \n",
        "    \"\"\"\n",
        "    Calls the /v1/model/load endpoint to load a model into TabbyAPI.\n",
        "    \"\"\"\n",
        "    url = f\"{url}/v1/model/load\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {TABBY_ADMIN_KEY}\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model_name\": model_name,\n",
        "        \"max_seq_len\": max_seq_len\n",
        "        # You can pass other parameters like \"gpu_split\" if desired\n",
        "    }\n",
        "\n",
        "    print(f\"Attempting to load model: {model_name}\")\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(f\"Successfully loaded model: {model_name}\")\n",
        "    else:\n",
        "        print(f\"Failed to load model. Response: {response.status_code} {response.text}\")\n",
        "        raise RuntimeError(\"Model load request failed.\")\n",
        "\n",
        "def unload_model(url):\n",
        "    \"\"\"\n",
        "    Calls the /v1/model/unload endpoint to load a model into TabbyAPI.\n",
        "    \"\"\"\n",
        "    url = f\"{url}/v1/model/unload\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {TABBY_ADMIN_KEY}\"\n",
        "    }\n",
        "    payload = {\n",
        "        # You can pass other parameters like \"gpu_split\" if desired\n",
        "    }\n",
        "\n",
        "    print(f\"Unloading TabbyAPI model\")\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(f\"Successfully unloaded model\")\n",
        "    else:\n",
        "        print(f\"Failed to unload model. Response: {response.status_code} {response.text}\")\n",
        "        raise RuntimeError(\"Model unload request failed.\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 2) Generate Text (OpenAI-Compatible Endpoint)\n",
        "# --------------------------------------------------------------------------\n",
        "def generate_text(url, prompt, model_name, max_tokens=100):\n",
        "    \"\"\"\n",
        "    Calls the /v1/completions endpoint to generate text from the loaded model.\n",
        "    \"\"\"\n",
        "    url = f\"{url}/v1/completions\"\n",
        "    # This can be a normal API token if you only want to *use* the loaded model,\n",
        "    # but if you expect to inline load a model here too, you need the admin token again.\n",
        "    # For safety, let's use the admin token in this example as well.\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {TABBY_ADMIN_KEY}\"\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        #\"model\": model_name,      # Must match the currently loaded model\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": 0.9\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    if response.status_code != 200:\n",
        "        raise RuntimeError(\n",
        "            f\"Generation request failed ({response.status_code}): {response.text}\"\n",
        "        )\n",
        "\n",
        "    result = response.json()\n",
        "    # The structure is similar to OpenAI:\n",
        "    # {\n",
        "    #   \"choices\": [{\"text\": \"...\"}],\n",
        "    #   \"id\": \"...\",\n",
        "    #   ...\n",
        "    # }\n",
        "    return result[\"choices\"][0][\"text\"]\n",
        "\n",
        "gpu_energy_store = [1, 1]\n",
        "use_second_gpu = use_multi_gpu_llm\n",
        "\n",
        "def gen_vr_content(in_javascript, base_prompt, extra_string):\n",
        "    global gpu_energy_store\n",
        "    gpu_index = 0\n",
        "    if use_second_gpu:\n",
        "        if gpu_energy_store[1] > gpu_energy_store[0]:\n",
        "            gpu_index = 1\n",
        "            gpu_energy_store[1] -= 60\n",
        "        else:\n",
        "            gpu_energy_store[0] -= 30\n",
        "\n",
        "        if gpu_energy_store[0] < 0 or gpu_energy_store[1] < 0:\n",
        "            gpu_energy_store[0] += 100\n",
        "            gpu_energy_store[1] += 100\n",
        "\n",
        "    \n",
        "    formatted_prompt = f\"\"\"You are a helpful assistant designing a VR world titled \"{base_prompt}\" {extra_string}\n",
        "[EXISTING_JAVASCRIPT]\n",
        "{in_javascript}\n",
        "[/EXISTING_JAVASCRIPT]\n",
        "\"\"\"\n",
        "    generation_output = generate_text(TABBY_BASE_URLS[gpu_index], formatted_prompt, MODEL_NAME, max_tokens=2048)\n",
        "\n",
        "    print(\"gpu_index\", gpu_index, gpu_energy_store)\n",
        "    # (Optional) print the prompt for debugging\n",
        "    print(\">>>>>>>>>>>>>>>>>>>>\", formatted_prompt)\n",
        "\n",
        "    # (Optional) print the result for debugging\n",
        "    print(\"<<<<<<<<<<<<<<<<<<<<\", generation_output)\n",
        "\n",
        "    return generation_output, formatted_prompt\n",
        "\n",
        "    # \n",
        "    # print(gen_vr_content(\"\"\"room(\"Cozy bedroom\").topLeft(0, 0).bottomRight(15, 15);\"\"\", \"Cozy Bedroom\"))\n",
        "\n",
        "if do_load_tabby_model and not do_vision_test_mode:\n",
        "    load_model(TABBY_BASE_URLS[0], MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the clip model\n",
        "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
        "devices = [\"cpu\"]\n",
        "if torch.cuda.is_available():\n",
        "    if use_multi_gpu_vision:\n",
        "        devices = ['cuda:0', 'cuda:1']\n",
        "    else:\n",
        "        devices = ['cuda:1']\n",
        "device_free = { }\n",
        "\n",
        "clip_model = {}\n",
        "clip_preprocess = {}\n",
        "\n",
        "def load_clip_model(\n",
        "    d,\n",
        "    model_name: str = \"ViT-H/14-378\",\n",
        "    pretrained: str = \"dfn5b\",\n",
        "    compile_model: bool = True\n",
        "):\n",
        "    # Create model and transform\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
        "    \n",
        "    # Optional PyTorch 2.x compile step\n",
        "    if compile_model:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # Move model to the device\n",
        "    model = model.to(d)\n",
        "    model.eval()\n",
        "\n",
        "    # Fill dictionaries\n",
        "    clip_model[d] = model\n",
        "    clip_preprocess[d] = preprocess\n",
        "\n",
        "\n",
        "def unload_clip_model(d):\n",
        "    clip_model[d] = None\n",
        "    clip_preprocess[d] = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "load_clip_model(\"cuda:0\")\n",
        "load_clip_model(\"cuda:1\")\n",
        "\n",
        "for i in range(100):\n",
        "    device_free[f\"cuda:{i}\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attempt to load the database\n",
        "num_model_loads = 0\n",
        "existing_image_features = None\n",
        "existing_text_features = None\n",
        "list_of_all_models = { }\n",
        "existing_searches_done = []\n",
        "index_to_model_uid = []\n",
        "num_models_downloaded = 0\n",
        "stack_existing_image_features = True\n",
        "image_ratings_data = []\n",
        "\n",
        "saved_model_data_filename = 'saved_model_data.json'\n",
        "saved_model_data_filename_backup = 'saved_model_data_backup.json'\n",
        "existing_image_features_filename = 'existing_image_features_big.pt'\n",
        "existing_text_features_filename = \"existing_text_features_big.pt\"\n",
        "existing_image_features_filename_backup = 'existing_image_features_backup.pt'\n",
        "existing_text_features_filename_backup = \"existing_text_features_backup.pt\"\n",
        "loaded_good = False\n",
        "\n",
        "try:\n",
        "    with open(saved_model_data_filename, 'r') as json_file:\n",
        "        saved_model_data = json.load(json_file)\n",
        "        existing_searches_done = saved_model_data[\"existing_searches_done\"]\n",
        "        list_of_all_models = saved_model_data[\"list_of_all_models\"]\n",
        "        index_to_model_uid = saved_model_data[\"index_to_model_uid\"]\n",
        "        \n",
        "        existing_image_features = torch.load(existing_image_features_filename)\n",
        "        print(\"existing_image_features shape:\", existing_image_features.shape)\n",
        "\n",
        "        existing_text_features = torch.load(existing_text_features_filename)\n",
        "        print(\"existing_text_features shape:\", existing_text_features.shape)\n",
        "\n",
        "        loaded_good = True\n",
        "        \n",
        "\n",
        "        print(\"index_to_model_uid length:\", len(index_to_model_uid))\n",
        "except Exception as e:\n",
        "    print(\"Exception\", e)    \n",
        "    with open(saved_model_data_filename_backup, 'r') as json_file:\n",
        "        saved_model_data = json.load(json_file)\n",
        "        existing_searches_done = saved_model_data[\"existing_searches_done\"]\n",
        "        list_of_all_models = saved_model_data[\"list_of_all_models\"]\n",
        "        index_to_model_uid = saved_model_data[\"index_to_model_uid\"]\n",
        "    \n",
        "    existing_image_features = torch.load(existing_image_features_filename_backup)\n",
        "    print(\"existing_image_features shape:\", existing_image_features.shape)\n",
        "\n",
        "    existing_text_features = torch.load(existing_text_features_filename_backup)\n",
        "    print(\"existing_text_features shape:\", existing_text_features.shape)\n",
        "    \n",
        "\n",
        "\n",
        "print(\"list_of_all_models\", len(list_of_all_models))\n",
        "\n",
        "#assert existing_image_features.shape == existing_text_features.shape == len(index_to_model_uid)\n",
        "\n",
        "print(len(index_to_model_uid))\n",
        "\n",
        "# Get the lengths of the Python lists\n",
        "len_models = len(index_to_model_uid)\n",
        "\n",
        "# Get the size along the first dimension (0th axis) of the PyTorch tensors\n",
        "len_image = existing_image_features.shape[0]\n",
        "len_text = existing_text_features.shape[0]\n",
        "\n",
        "# Find the minimum length\n",
        "shortest_len = min(len_models, len_image, len_text)\n",
        "\n",
        "print(\"shortest_len\", shortest_len)\n",
        "\n",
        "# If any of them doesn't match, truncate them all to the shortest length\n",
        "if not (len_models == len_image == len_text):\n",
        "    print(f\"Mismatch found. Truncating all arrays/tensors to shortest length = {shortest_len}.\")\n",
        "\n",
        "    # Truncate the Python lists\n",
        "    index_to_model_uid = index_to_model_uid[:shortest_len]\n",
        "    existing_searches_done = existing_searches_done[:shortest_len]\n",
        "    index_to_model_uid = index_to_model_uid[:shortest_len]\n",
        "\n",
        "    # Truncate the PyTorch tensors\n",
        "    existing_image_features = existing_image_features[:shortest_len]\n",
        "    existing_text_features = existing_text_features[:shortest_len]\n",
        "\n",
        "# Now they should all match\n",
        "assert len(index_to_model_uid) == existing_image_features.shape[0] == existing_text_features.shape[0]\n",
        "\n",
        "if loaded_good:\n",
        "    with open(saved_model_data_filename_backup, \"w\") as f:\n",
        "        saved_model_data = { \"existing_searches_done\": existing_searches_done, \"list_of_all_models\": list_of_all_models, \"index_to_model_uid\": index_to_model_uid }\n",
        "        json.dump(saved_model_data, f)\n",
        "    torch.save(existing_image_features.cpu().detach(), existing_image_features_filename_backup)\n",
        "    torch.save(existing_text_features.cpu().detach(), existing_text_features_filename_backup)\n",
        "\n",
        "\n",
        "print(\"All arrays/tensors have been truncated (if needed) and are now consistent.\")\n",
        "# except Exception as e:\n",
        "#     print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def addSearchableModel(uid, image_url, name):\n",
        "    global existing_image_features\n",
        "    global existing_text_features\n",
        "    global num_model_loads\n",
        "\n",
        "    d_zero = \"cuda:1\"\n",
        "\n",
        "    if not uid in list_of_all_models:\n",
        "        response = requests.get(image_url)\n",
        "        if response.status_code == 200:\n",
        "            image = Image.open(BytesIO(response.content))\n",
        "\n",
        "            if image.width < min_image_size or image.height < min_image_size:\n",
        "                return\n",
        "            \n",
        "            tokenized_text = open_clip.tokenize(name).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                new_text_features = clip_model[d_zero].encode_text(tokenized_text).detach()\n",
        "                \n",
        "            # Normalize the features (optional but recommended)\n",
        "            new_text_features = new_text_features / new_text_features.norm(dim=-1, keepdim=True)\n",
        "            new_text_features_np = new_text_features.to(device).detach()\n",
        "\n",
        "            if existing_text_features is not None:\n",
        "                existing_text_features = torch.cat([existing_text_features.to(device), new_text_features_np], dim=0)\n",
        "            else:\n",
        "                existing_text_features = new_text_features_np\n",
        "            \n",
        "            new_text_features_np.detach()\n",
        "            del new_text_features\n",
        "            del new_text_features_np\n",
        "\n",
        "            # image checking       \n",
        "            image_tensor = clip_preprocess[d_zero](image).to(device).detach().unsqueeze(0)\n",
        "            \n",
        "            # image_batch = torch.cat(image_tensor).to(device)  # Combine images into a batch\n",
        "            image_features = clip_model[d_zero].encode_image(image_tensor).to(device).detach()\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize features\n",
        "            new_image_features_np = image_features.to(device).detach()\n",
        "\n",
        "            if existing_image_features is not None:\n",
        "                existing_image_features = torch.cat([existing_image_features.to(device), new_image_features_np], dim=0)\n",
        "            else:\n",
        "                existing_image_features = new_image_features_np\n",
        "            \n",
        "            list_of_all_models[uid] = { \"name\": name, \"image\": image_url, \"uid\": uid }\n",
        "            index_to_model_uid.append(uid)\n",
        "            num_model_loads += 1\n",
        "            \n",
        "            del new_image_features_np\n",
        "            del image\n",
        "            del image_tensor\n",
        "            del image_features\n",
        "\n",
        "            # Trigger garbage collection and print uncollectable objects\n",
        "            gc.collect()\n",
        "\n",
        "            torch.cuda.empty_cache()  # Clear GPU memory\n",
        "            print(\"addSearchableModel\", name, uid, image_url)\n",
        "            \n",
        "            with open(\"saved_model_data.json\", \"w\") as f:\n",
        "                saved_model_data = { \"existing_searches_done\": existing_searches_done, \"list_of_all_models\": list_of_all_models, \"index_to_model_uid\": index_to_model_uid }\n",
        "                json.dump(saved_model_data, f)\n",
        "            torch.save(existing_image_features.cpu().detach(), existing_image_features_filename)\n",
        "            torch.save(existing_text_features.cpu().detach(), existing_text_features_filename)\n",
        "        else:\n",
        "            print(f\"Failed to fetch {image_url}. Status code: {response.status_code}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_js_blocks(text, doJoin = True):\n",
        "    pattern = r\"```(?:js|javascript)(.*?)```\"\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    unique_matches = set(match.replace(\";\", \"\").strip() for match in matches)\n",
        "    if len(unique_matches) == 0:\n",
        "        pattern = r\"```(.*?)```\"\n",
        "        matches = re.findall(pattern, text, re.DOTALL)\n",
        "        unique_matches = set(match.replace(\";\", \"\").strip() for match in matches)\n",
        "\n",
        "    if doJoin:\n",
        "        return \"\\n\".join(unique_matches)\n",
        "    else:\n",
        "        return unique_matches\n",
        "\n",
        "def remove_js_comments(text):\n",
        "    pattern = r'(?<![\"\\':/])//.*$'\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = [re.sub(pattern, '', line) for line in lines]\n",
        "    return '\\n'.join(line.rstrip() for line in cleaned_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vision_data = []\n",
        "try:\n",
        "    with open('./model_outputs_llama8b_vision.json', 'r') as file:\n",
        "        vision_data = json.load(file)\n",
        "        print(len(vision_data['data']))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "modelsDownloaded = { }\n",
        "existing_models_searched = { }\n",
        "\n",
        "# respect users who don't want their models used in any any related things\n",
        "def hasNoAITag(model):\n",
        "    if model[\"tags\"]:\n",
        "        for tagIter in model[\"tags\"]:\n",
        "            if \"noai\" in tagIter[\"name\"].lower():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def getModelList(objectName):\n",
        "\n",
        "    # Specify the URL you want to request\n",
        "    url = \"https://api.sketchfab.com/v3/search?type=models&q=\" + objectName + \"&downloadable=true&min_face_count=\" + str(minModelFaces) +  \"&max_face_count=\" + str(maxModelFaces) +  \"&archives_flavours=false\"\n",
        "    print(\"url\", url)\n",
        "\n",
        "    # Send an HTTPS GET request\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 429:\n",
        "        print(\"response.status_code == 429 sleeping for a minute\")\n",
        "        return \"\"    \n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        data = json.loads(response.text)\n",
        "\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            data = json.loads(response.text)\n",
        "            return_data = []\n",
        "            if data and data[\"results\"] and data[\"results\"][0]:\n",
        "                for iter in data[\"results\"]:\n",
        "                    if not hasNoAITag(iter):\n",
        "                        return_data.append(iter)\n",
        "                        addSearchableModel(iter[\"uid\"], iter[\"thumbnails\"][\"images\"][0][\"url\"], iter[\"name\"])\n",
        "                        \n",
        "                return return_data\n",
        "            \n",
        "        else:\n",
        "            print(f\"Failed to fetch {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    return []\n",
        "\n",
        "def download_one_model(model, depth):\n",
        "    global num_model_loads\n",
        "\n",
        "    objectName = model[\"name\"]\n",
        "    modelUID = model[\"uid\"]\n",
        "    baseDir = model_download_path\n",
        "    print(\"os.mkdir(baseDir): \" + baseDir)\n",
        "\n",
        "    if not os.path.exists(\"zipped_models\"):\n",
        "        os.mkdir(\"zipped_models\")\n",
        "    if not os.path.exists(baseDir):\n",
        "        os.mkdir(baseDir)\n",
        "\n",
        "    object_name_plus_depth = objectName + \"_\" + str(depth)\n",
        "\n",
        "    unzipped_file_path = baseDir + os.sep + modelUID\n",
        "    \n",
        "    if os.path.exists(unzipped_file_path):\n",
        "        print(\"os.path.exists(unzipped_file_path): \" + unzipped_file_path)\n",
        "        modelsDownloaded[object_name_plus_depth] = modelUID\n",
        "        return modelUID\n",
        "\n",
        "    zipped_file_path = \"zipped_models\" + os.sep + modelUID + \".zip\"\n",
        "\n",
        "    if os.path.exists(zipped_file_path):\n",
        "        with zipfile.ZipFile(zipped_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(unzipped_file_path)\n",
        "            print(\"os.path.exists(file_path): \" + zipped_file_path)\n",
        "        modelsDownloaded[object_name_plus_depth] = modelUID\n",
        "        \n",
        "        return modelUID\n",
        "\n",
        "    url = \"https://api.sketchfab.com/v3/models/\" + model[\"uid\"] + \"/download\"\n",
        "    headers = {\n",
        "        'Authorization': f\"Token {sketchfab_api_token}\"\n",
        "    }\n",
        "    params = {\n",
        "        'mode': 'cors'\n",
        "    }\n",
        "    \n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    modelData = json.loads(response.text)\n",
        "            \n",
        "    # Specify the URL of the file you want to download\n",
        "    print(\"modelData\", modelData, \"response\", response)\n",
        "\n",
        "    if not \"gltf\" in modelData:\n",
        "        return None\n",
        "\n",
        "    file_url = modelData[\"gltf\"]['url']\n",
        "    print(file_url)\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(file_url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Get the content of the response and save it to a local file\n",
        "        with open(zipped_file_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "            \n",
        "            # Open the zip file\n",
        "            with zipfile.ZipFile(zipped_file_path, 'r') as zip_ref:\n",
        "                \n",
        "                zip_ref.extractall(unzipped_file_path)\n",
        "                print(\"Extracted\", unzipped_file_path)\n",
        "                modelsDownloaded[objectName] = modelUID\n",
        "                \n",
        "            num_model_loads += 1\n",
        "            return modelUID\n",
        "    else:\n",
        "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "        \n",
        "def zscore(similarity):\n",
        "    return (similarity - similarity.mean()) / similarity.std()\n",
        "\n",
        "with open('generate_from_llama8_bad_models.txt', 'r') as file:\n",
        "    baaaaad_models = file.read().split('\\n')\n",
        "    print(\"baaaaad_models\", len(baaaaad_models))\n",
        "\n",
        "stored_model_vec_searches = { }\n",
        "def load_model_from_vec(objectName, model_depth):\n",
        "    d_zero = devices[0]\n",
        "\n",
        "    if objectName in stored_model_vec_searches:\n",
        "        top_X_models = stored_model_vec_searches[objectName]\n",
        "    else:\n",
        "        pos_prompts = [objectName, \"cohesive attractive design\", 'high poly high quality', 'attractive and pretty colors', 'cohesive single object', 'object with a solid base']\n",
        "        pos_weights=[1.1, 0.2, 0.2, 0.2, 0.4, 0.2]\n",
        "    \n",
        "        neg_prompts=[f\"object in a room\", \"a complete room\", 'ugly or low quality', 'low poly', 'abstract and disconnected', 'black and white', 'greyscale', 'indoor room', 'small room within a room', 'room within a room', \"inverted normals\", 'distorded, floating, or confused object'] \n",
        "        neg_weights=[0.3, 0.3, 0.3, 0.3, 0.2, 0.1, 0.1, 0.2, 0.2, 0.2, 0.3, 0.3]\n",
        "        \n",
        "        print(\"load_model_from_vec - Positive Prompts:\", pos_prompts)\n",
        "        print(\"load_model_from_vec - Negative Prompts:\", neg_prompts)\n",
        "        \n",
        "        # ----- Compute positive embeddings -----\n",
        "        pos_tokens = open_clip.tokenize(pos_prompts).to(device)\n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=\"cuda:0\", dtype=torch.float16):\n",
        "            pos_embeddings = clip_model[d_zero].encode_text(pos_tokens).to(device)\n",
        "            pos_embeddings = pos_embeddings / pos_embeddings.norm(dim=-1, keepdim=True)\n",
        "        \n",
        "        # Weight and combine the positive embeddings.\n",
        "        weighted_pos = torch.stack([w * emb for w, emb in zip(pos_weights, pos_embeddings)], dim=0)\n",
        "        combined_pos = weighted_pos.sum(dim=0)\n",
        "        \n",
        "        # ----- Compute negative embeddings (if any) -----\n",
        "        if neg_prompts:\n",
        "            neg_tokens = open_clip.tokenize(neg_prompts).to(device)\n",
        "            \n",
        "            with torch.no_grad(), torch.amp.autocast(device_type=\"cuda:0\", dtype=torch.float16):\n",
        "                neg_embeddings = clip_model[d_zero].encode_text(neg_tokens).to(device)\n",
        "                neg_embeddings = neg_embeddings / neg_embeddings.norm(dim=-1, keepdim=True)\n",
        "            weighted_neg = torch.stack([w * emb for w, emb in zip(neg_weights, neg_embeddings)], dim=0)\n",
        "            combined_neg = weighted_neg.sum(dim=0)\n",
        "        else:\n",
        "            # If no negative prompts are provided, use a zero tensor of the same shape.\n",
        "            combined_neg = torch.zeros_like(combined_pos)\n",
        "        \n",
        "        # ----- Combine positive and negative embeddings -----\n",
        "        combined_text = combined_pos - combined_neg\n",
        "        combined_text = combined_text / combined_text.norm()\n",
        "        combined_text = combined_text.to(device)\n",
        "        \n",
        "        # ----- Compute similarity with existing image features -----\n",
        "        # (Assuming existing_image_features is a [N x D] tensor where D is the embedding dim.)\n",
        "        combined_similarity = (existing_image_features.to(device) @ combined_text.T).squeeze()\n",
        "        combined_similarity = zscore(combined_similarity.cpu().numpy())\n",
        "\n",
        "        # Get the best match index based on the combined similarity\n",
        "        num_top_models = len(combined_similarity)\n",
        "        \n",
        "        top_X_indices = np.argsort(-combined_similarity)[:10000]\n",
        "        top_X_models = [list_of_all_models[index_to_model_uid[idx]] for idx in top_X_indices]\n",
        "        stored_model_vec_searches[objectName] = top_X_models\n",
        "        \n",
        "    gc.collect()  # Run garbage collection\n",
        "    torch.cuda.empty_cache()  # Clear GPU memory\n",
        "\n",
        "\n",
        "    base_depth = model_depth\n",
        "\n",
        "    for one_model in top_X_models:\n",
        "        if one_model['uid'] in baaaaad_models:\n",
        "            continue\n",
        "\n",
        "        if base_depth > 0 and not download_depth_models:\n",
        "            unzipped_file_path = model_download_path + os.sep + one_model[\"uid\"]\n",
        "            if not os.path.exists(unzipped_file_path):\n",
        "                continue\n",
        "    \n",
        "        modelUID = one_model[\"uid\"]\n",
        "        best_model = one_model\n",
        "        model_depth -= 1\n",
        "        if model_depth < 0:\n",
        "            break\n",
        "    \n",
        "    if not \"loaded\" in best_model:\n",
        "        download_one_model(best_model, base_depth)\n",
        "    \n",
        "    modelsDownloaded[f\"objectName_{base_depth}\"] = modelUID\n",
        "    \n",
        "\n",
        "    unzipped_file_path = model_download_path + os.sep + modelUID\n",
        "    if not os.path.exists(unzipped_file_path):\n",
        "        for one_model in top_X_models:\n",
        "            if not os.path.exists(unzipped_file_path):\n",
        "                continue\n",
        "    \n",
        "            modelUID = one_model[\"uid\"]\n",
        "            break\n",
        "\n",
        "    return modelUID\n",
        "\n",
        "\n",
        "def downloadModel(objectName, model_depth):\n",
        "    global existing_models_searched\n",
        "    \n",
        "    room_plus_object = objectName\n",
        "    depth_plus_obj = f\"{objectName}_{model_depth}\"\n",
        "\n",
        "    if depth_plus_obj in existing_models_searched:\n",
        "        return existing_models_searched[depth_plus_obj]\n",
        "\n",
        "    if False:\n",
        "        if f\"objectName_{model_depth}\" in modelsDownloaded:\n",
        "            print(\"return modelsDownloaded[objectName]\" + \"_\" + str(model_depth), objectName)\n",
        "            download_one_model({ \"uid\": modelsDownloaded[objectName], 'name': objectName})\n",
        "            return modelsDownloaded[objectName]\n",
        "\n",
        "    if objectName in existing_searches_done or freeze_model_database_models:\n",
        "        the_result = load_model_from_vec(room_plus_object, model_depth)\n",
        "        existing_models_searched[depth_plus_obj] = the_result\n",
        "        return the_result\n",
        "    existing_searches_done.append(objectName)\n",
        "\n",
        "    getModelList(objectName)\n",
        "    modelTokens = objectName.split(' ')\n",
        "    if False:\n",
        "        if len(modelTokens) > 5:\n",
        "            subDesc = askMultiQuestion([f\"If I was searching in a model database for \\\"{objectName}\\\" and could only search using five words, which words should I use? Avoid using plural words.\", \"Say the words.  Just say five words.  Don't say anything else.  Just five.  That's it.\"])[0].strip().replace(\"\\\"\", \"\").replace(\"\\'\",\"\")\n",
        "            if subDesc not in existing_searches_done:\n",
        "                existing_searches_done.append(subDesc)\n",
        "                getModelList(subDesc)\n",
        "\n",
        "        if len(modelTokens) > 4:\n",
        "            subDesc = askMultiQuestion([f\"If I was searching in a model database for \\\"{objectName}\\\" and could only search using four words, which words should I use? Avoid using plural words.\", \"Say the words.  Just say four words.  Don't say anything else.  Just four.  That's it.\"])[0].strip().replace(\"\\\"\", \"\").replace(\"\\'\",\"\")\n",
        "            if subDesc not in existing_searches_done:\n",
        "                existing_searches_done.append(subDesc)\n",
        "                getModelList(subDesc)\n",
        "\n",
        "        if len(modelTokens) > 3:\n",
        "            subDesc = askMultiQuestion([f\"If I was searching in a model database for \\\"{objectName}\\\" and could only search using three words, which words should I use? Avoid using plural words.\", \"Say the words.  Just say three words.  Don't say anything else.  Just three.  That's it.\"])[0].strip().replace(\"\\\"\", \"\").replace(\"\\'\",\"\")\n",
        "            if subDesc not in existing_searches_done:\n",
        "                existing_searches_done.append(subDesc)\n",
        "                getModelList(subDesc)\n",
        "\n",
        "        if len(modelTokens) > 2:\n",
        "            subDesc = askMultiQuestion([f\"If I was searching in a model database for \\\"{objectName}\\\" and could only search using two words, which words should I use? Avoid using plural words.\", \"Say the words.  Just say two words.  Don't say anything else.  Just two.  That's it.\"])[0].strip().replace(\"\\\"\", \"\").replace(\"\\'\",\"\")\n",
        "            if subDesc not in existing_searches_done:\n",
        "                existing_searches_done.append(subDesc)\n",
        "                getModelList(subDesc)\n",
        "    \n",
        "    for subDesc in modelTokens:\n",
        "        if subDesc not in existing_searches_done:\n",
        "            existing_searches_done.append(subDesc)\n",
        "            getModelList(subDesc)\n",
        "    \n",
        "    the_result = load_model_from_vec(room_plus_object, model_depth)\n",
        "    existing_models_searched[depth_plus_obj] = the_result\n",
        "    return the_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "from torch.cuda.amp import autocast\n",
        "import torch.backends.cudnn as cudnn\n",
        "cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False \n",
        "\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import open_clip\n",
        "import win32api, win32process, win32con\n",
        "import threading\n",
        "\n",
        "threading_lock = threading.Lock()\n",
        "\n",
        "def find_similar_images_with_negatives_threaded(\n",
        "    pos_prompts, \n",
        "    neg_prompts, \n",
        "    image_folder,   \n",
        "    model,     \n",
        "    preprocess, \n",
        "    pos_weights, \n",
        "    neg_weights, \n",
        "    device,\n",
        "    batch_size,\n",
        "    group_similarities,\n",
        "    images_already_seen,\n",
        "    timeout_after_batch\n",
        "):\n",
        "    global baseline_image_features\n",
        "    global total_num_variations_per_steps\n",
        "    if pos_prompts:\n",
        "        pos_tokens = open_clip.tokenize(pos_prompts).to(device)\n",
        "      \n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
        "            pos_embeddings = model.encode_text(pos_tokens)\n",
        "            pos_embeddings = pos_embeddings / pos_embeddings.norm(dim=-1, keepdim=True)\n",
        "        # Multiply each embedding by its corresponding weight.\n",
        "        weighted_pos = torch.stack([w * emb for w, emb in zip(pos_weights, pos_embeddings)], dim=0)\n",
        "        combined_pos = weighted_pos.sum(dim=0)\n",
        "    else:\n",
        "        combined_pos = 0\n",
        "\n",
        "    print(\"pos_prompts:\", pos_prompts)\n",
        "\n",
        "    # --- Compute negative text embeddings if any negative prompts exist ---\n",
        "    if neg_prompts:\n",
        "        neg_tokens = open_clip.tokenize(neg_prompts).to(device)\n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
        "            neg_embeddings = model.encode_text(neg_tokens)\n",
        "            neg_embeddings = neg_embeddings / neg_embeddings.norm(dim=-1, keepdim=True)\n",
        "        weighted_neg = torch.stack([w * emb for w, emb in zip(neg_weights, neg_embeddings)], dim=0)\n",
        "        combined_neg = weighted_neg.sum(dim=0)\n",
        "    else:\n",
        "        combined_neg = 0\n",
        "\n",
        "    print(\"neg_prompts:\", neg_prompts)\n",
        "\n",
        "    # --- Combine positive and negative embeddings ---\n",
        "    combined_text = combined_pos - combined_neg\n",
        "    # combined_text = combined_text * 0.95 + baseline_image_features.to(device) * 0.05\n",
        "    combined_text = combined_text / combined_text.norm()\n",
        "\n",
        "    num_sleeps = 0\n",
        "    prev_num_files = 0\n",
        "\n",
        "    def process_batch(image_batch, group_ids_batch):\n",
        "        global device_free\n",
        "        device_free[device] = False\n",
        "        print(\"process_batch\", len(image_batch), device)\n",
        "      \n",
        "        if not image_batch:\n",
        "            return\n",
        "\n",
        "        # Stack all images in the batch into one tensor\n",
        "        image_input = torch.stack(image_batch, dim=0).to(device)\n",
        "\n",
        "        # Compute image features for the entire batch\n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
        "            image_features = model.encode_image(image_input)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute similarity for each image in the batch\n",
        "        similarities = (combined_text.unsqueeze(0) @ image_features.T).squeeze(0)\n",
        "        # similarities will have shape [batch_size]\n",
        "\n",
        "        for gid, sim in zip(group_ids_batch, similarities):\n",
        "            if gid not in group_similarities:\n",
        "                group_similarities[gid] = []\n",
        "            group_similarities[gid].append(sim.item())\n",
        "\n",
        "        print(\"process_batch_finish\", len(image_batch), device)\n",
        "        time.sleep(timeout_after_batch)\n",
        "        device_free[device] = True\n",
        "\n",
        "    prev_num_sims = 0\n",
        "\n",
        "    num_images = 0\n",
        "    image_batch = []\n",
        "    group_ids_batch = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            #print(\"prev_num_sims\", prev_num_sims, \"len(group_similarities)\", len(group_similarities), \"num_sleeps\", num_sleeps)\n",
        "            if len(image_batch) == 0:\n",
        "                if len(group_similarities) >= total_num_variations_per_steps:\n",
        "                    break\n",
        "                elif prev_num_sims == len(group_similarities):\n",
        "                    num_files = len(os.listdir(image_folder))\n",
        "                    if prev_num_files > 0:\n",
        "                        print(\"num_sleeps\", num_sleeps, \"prev_num_files\", prev_num_files, \"num_files\", num_files)\n",
        "                    if num_files == prev_num_files and prev_num_files > 0:\n",
        "                        num_sleeps += 1\n",
        "                        if num_sleeps >= max_sleeps:\n",
        "                            break\n",
        "                    else:\n",
        "                        num_sleeps = 0\n",
        "                    prev_num_files = num_files\n",
        "\n",
        "                # If we reached the batch size, process and reset\n",
        "            if len(image_batch) < batch_size:\n",
        "                for filename in os.listdir(image_folder):\n",
        "                    \n",
        "                    filepath = os.path.join(image_folder, filename)\n",
        "                    # Skip non-image files.\n",
        "                    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "                        continue\n",
        "                    \n",
        "                    # Extract group ID from filename: e.g., \"xxxx_123.png\" -> group_id = \"xxxx\"\n",
        "                    base_name, _ = os.path.splitext(filename)  \n",
        "                    if \"_\" not in base_name:\n",
        "                        # If there's no underscore, skip or treat differently\n",
        "                        continue\n",
        "                    \n",
        "                    with threading_lock:\n",
        "                        if filename in images_already_seen:\n",
        "                            continue\n",
        "\n",
        "                        images_already_seen.add(filename)\n",
        "\n",
        "                    group_id = base_name.split(\"_\")[0]\n",
        "\n",
        "                    # Load and preprocess the image.\n",
        "                    image = Image.open(filepath).convert(\"RGB\")\n",
        "                    image_tensor = preprocess(image)\n",
        "                    \n",
        "                    # Add to our batch\n",
        "                    image_batch.append(image_tensor)\n",
        "                    group_ids_batch.append(group_id)\n",
        "                    print(\"num_images\", num_images, \"images_already_seen\", len(images_already_seen), \"group_similarities\", len(group_similarities))\n",
        "                    num_images += 1\n",
        "\n",
        "                    # If we reached the batch size, process and reset\n",
        "                    if len(image_batch) >= batch_size:\n",
        "                        break\n",
        "\n",
        "            if len(image_batch) > 0 and device_free[device]:\n",
        "                process_batch(image_batch, group_ids_batch)\n",
        "                num_images = 0\n",
        "                image_batch = []\n",
        "                group_ids_batch = []\n",
        "\n",
        "            prev_num_sims = len(group_similarities)\n",
        "        except Exception as e:\n",
        "            print(\"Exception\", e)\n",
        "        time.sleep(0.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def find_similar_images_with_negatives(\n",
        "    pos_prompts, \n",
        "    neg_prompts, \n",
        "    image_folder, \n",
        "    model, \n",
        "    preprocess, \n",
        "    pos_weights=None, \n",
        "    neg_weights=None, \n",
        "    device=\"gpu\",\n",
        "    top_k=None,\n",
        "    batch_size= 250  # new argument to control batch size\n",
        "):  \n",
        "    # Default weights: 1.0 for all prompts if not provided.\n",
        "    if pos_weights is None:\n",
        "        pos_weights = [1.0] * len(pos_prompts)\n",
        "    if neg_weights is None:\n",
        "        neg_weights = [1.0] * len(neg_prompts)\n",
        "    \n",
        "    # --- Compute positive text embeddings if any positive prompts exist ---\n",
        "    if pos_prompts:\n",
        "        pos_tokens = open_clip.tokenize(pos_prompts).to(device)\n",
        "      \n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
        "            pos_embeddings = model.encode_text(pos_tokens)\n",
        "            pos_embeddings = pos_embeddings / pos_embeddings.norm(dim=-1, keepdim=True)\n",
        "        # Multiply each embedding by its corresponding weight.\n",
        "        weighted_pos = torch.stack([w * emb for w, emb in zip(pos_weights, pos_embeddings)], dim=0)\n",
        "        combined_pos = weighted_pos.sum(dim=0)\n",
        "    else:\n",
        "        combined_pos = 0\n",
        "\n",
        "    print(\"pos_prompts:\", pos_prompts)\n",
        "\n",
        "    # --- Compute negative text embeddings if any negative prompts exist ---\n",
        "    if neg_prompts:\n",
        "        neg_tokens = open_clip.tokenize(neg_prompts).to(device)\n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
        "            neg_embeddings = model.encode_text(neg_tokens)\n",
        "            neg_embeddings = neg_embeddings / neg_embeddings.norm(dim=-1, keepdim=True)\n",
        "        weighted_neg = torch.stack([w * emb for w, emb in zip(neg_weights, neg_embeddings)], dim=0)\n",
        "        combined_neg = weighted_neg.sum(dim=0)\n",
        "    else:\n",
        "        combined_neg = 0\n",
        "\n",
        "    print(\"neg_prompts:\", neg_prompts)\n",
        "\n",
        "    # --- Combine positive and negative embeddings ---\n",
        "    combined_text = combined_pos - combined_neg\n",
        "    combined_text = combined_text / combined_text.norm()\n",
        "\n",
        "    # Dictionary to store {group_id: [list_of_similarities]}\n",
        "    group_similarities = {}\n",
        "\n",
        "    # --- Prepare to process images in batches ---\n",
        "    image_batch = []\n",
        "    group_ids_batch = []\n",
        "    num_images = 0\n",
        "\n",
        "    def process_batch(image_batch, group_ids_batch):\n",
        "        print(\"process_batch\")\n",
        "        \"\"\"\n",
        "        Encodes images in the batch, computes similarity, \n",
        "        and updates group_similarities.\n",
        "        \"\"\"\n",
        "        if not image_batch:\n",
        "            return\n",
        "\n",
        "        # Stack all images in the batch into one tensor\n",
        "        image_input = torch.stack(image_batch, dim=0).to(device)\n",
        "\n",
        "        # Compute image features for the entire batch\n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
        "            image_features = model.encode_image(image_input)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute similarity for each image in the batch\n",
        "        similarities = (combined_text.unsqueeze(0) @ image_features.T).squeeze(0)\n",
        "        # similarities will have shape [batch_size]\n",
        "\n",
        "        for gid, sim in zip(group_ids_batch, similarities):\n",
        "            if gid not in group_similarities:\n",
        "                group_similarities[gid] = []\n",
        "            group_similarities[gid].append(sim.item())\n",
        "\n",
        "    # --- Iterate over all images in the specified folder ---\n",
        "    num_images = 0\n",
        "    for filename in os.listdir(image_folder):\n",
        "        filepath = os.path.join(image_folder, filename)\n",
        "        # Skip non-image files.\n",
        "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "            continue\n",
        "        \n",
        "        # Extract group ID from filename: e.g., \"xxxx_123.png\" -> group_id = \"xxxx\"\n",
        "        base_name, _ = os.path.splitext(filename)  \n",
        "        if \"_\" not in base_name:\n",
        "            # If there's no underscore, skip or treat differently\n",
        "            continue\n",
        "        group_id = base_name.split(\"_\")[0]\n",
        "\n",
        "        # Load and preprocess the image.\n",
        "        image = Image.open(filepath).convert(\"RGB\")\n",
        "        image_tensor = preprocess(image)\n",
        "        \n",
        "        # Add to our batch\n",
        "        image_batch.append(image_tensor)\n",
        "        group_ids_batch.append(group_id)\n",
        "        print(\"num_images\", num_images)\n",
        "        num_images += 1\n",
        "\n",
        "        # If we reached the batch size, process and reset\n",
        "        if len(image_batch) == batch_size:\n",
        "            process_batch(image_batch, group_ids_batch)\n",
        "            image_batch = []\n",
        "            group_ids_batch = []\n",
        "\n",
        "    # Process any leftover images in the last batch\n",
        "    if len(image_batch) > 0:\n",
        "        process_batch(image_batch, group_ids_batch)\n",
        "\n",
        "    # --- Compute average similarity for each group ---\n",
        "    group_avg_similarities = []\n",
        "    for gid, sims in group_similarities.items():\n",
        "        avg_sim = sum(sims) / len(sims)\n",
        "        group_avg_similarities.append((gid, avg_sim))\n",
        "\n",
        "    # --- Sort groups by average similarity (descending) ---\n",
        "    group_avg_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # --- If top_k is specified, truncate to top_k groups ---\n",
        "    if top_k is not None:\n",
        "        group_avg_similarities = group_avg_similarities[:top_k]\n",
        "\n",
        "    print(f\"Processed {num_images} images in total.\")\n",
        "    return group_avg_similarities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def modify_function_calls(input_string, random_iter):\n",
        "    # Define which function names we want to modify\n",
        "    # Add more function names here if needed (e.g. 'clone', 'size', 'rotate')\n",
        "    keywords = ['.rotate']\n",
        "    \n",
        "    # Create a single pattern that matches any of the keywords followed by (...)\n",
        "    pattern = rf'({\"|\".join(keywords)})\\(\\s*([^)]*)\\)'\n",
        "    \n",
        "    def randomize_numbers(match):\n",
        "        \n",
        "        \"\"\"\n",
        "        This function is called for every match (e.g., position(2, 0, 2)).\n",
        "        It will parse out the numbers, add a random offset, and return\n",
        "        the modified string.\n",
        "        \"\"\"\n",
        "        func_name = match.group(1)   # e.g. 'position'\n",
        "\n",
        "        args_str  = match.group(2)   # e.g. '2, 0, 2'\n",
        "        \n",
        "        # Split arguments by comma, strip spaces\n",
        "        args = [arg.strip() for arg in args_str.split(',')]\n",
        "        \n",
        "        arg_index = 0\n",
        "\n",
        "        # For each argument, convert to float, add random offset\n",
        "        new_args = []\n",
        "        for arg in args:\n",
        "            try:\n",
        "                val = float(arg)\n",
        "                if func_name == '.rotate':\n",
        "                    val = random_iter * 90\n",
        "                new_args.append(str(int(val)))\n",
        "            except ValueError:\n",
        "                # If it's not a float, just keep it as-is\n",
        "                new_args.append(arg)\n",
        "            arg_index += 1\n",
        "        \n",
        "        # Reconstruct the function call with modified numbers\n",
        "        new_args_str = ', '.join(new_args)\n",
        "        return f\"{func_name}({new_args_str})\"\n",
        "    \n",
        "    # Perform the substitution on the entire string\n",
        "    modified_string = re.sub(pattern, randomize_numbers, input_string)\n",
        "    if \".rotate\" not in modified_string:\n",
        "        modified_string = modified_string.replace(\")\\n\", f').rotate({str(int(random_iter) * 90)})\\n')\n",
        "\n",
        "    return modified_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "directory_path = \"./node_webgl_render_agentic/screenshots\"\n",
        "\n",
        "def clear_screenshot_dir():\n",
        "    if not do_clear_screenshot_dir:\n",
        "        return\n",
        "    \n",
        "    if do_vision_test_mode:\n",
        "        return\n",
        "    \n",
        "    run_without_error = False\n",
        "    while not run_without_error:\n",
        "        try:\n",
        "            for file_name in os.listdir(directory_path):\n",
        "                file_path = os.path.join(directory_path, file_name)\n",
        "                if os.path.isfile(file_path):  # Ensure it's a file\n",
        "                    os.remove(file_path)\n",
        "            run_without_error = True\n",
        "        except Exception as e:\n",
        "            print(\"Exception\", e)\n",
        "\n",
        "\n",
        "with open('./node_webgl_render_agentic/world_text.json', 'w') as file:\n",
        "    the_output_json = { 'options': [\"\"], 'prompt': f\"none\" }\n",
        "    json.dump(the_output_json, file)\n",
        "clear_screenshot_dir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_baseline(device, image_folder, model, preprocess, base_javascript = \"\"\"room(\"Baseline\").topLeft(0, 0).bottomRight(15, 15);\"\"\"):\n",
        "    clear_screenshot_dir()\n",
        "    with open('./node_webgl_render_agentic/world_text.json', 'w') as file:\n",
        "        the_output_json = { 'options': [base_javascript], 'prompt': f\"baseline\" }\n",
        "        json.dump(the_output_json, file)\n",
        "    while True:\n",
        "        image_files = os.listdir(image_folder)\n",
        "        if len(image_files) < num_images_per_variation:\n",
        "            time.sleep(0.1)\n",
        "            continue\n",
        "        image_batch = []\n",
        "        for filename in image_files:\n",
        "            filepath = os.path.join(image_folder, filename)\n",
        "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "                continue\n",
        "            \n",
        "            # Extract group ID from filename: e.g., \"xxxx_123.png\" -> group_id = \"xxxx\"\n",
        "            base_name, _ = os.path.splitext(filename)  \n",
        "            if \"_\" not in base_name:\n",
        "                # If there's no underscore, skip or treat differently\n",
        "                continue\n",
        "\n",
        "            image = Image.open(filepath).convert(\"RGB\")\n",
        "            image_tensor = preprocess(image)\n",
        "            image_batch.append(image_tensor)\n",
        "        \n",
        "        # Stack all images in the batch into one tensor\n",
        "        image_input = torch.stack(image_batch, dim=0).to(device)\n",
        "\n",
        "        # Compute image features for the entire batch\n",
        "        with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
        "            image_features = model.encode_image(image_input)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        \n",
        "        avg_image_features = image_features.mean(dim=0)\n",
        "        avg_image_features = avg_image_features / avg_image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return avg_image_features\n",
        "\n",
        "d = \"cuda:1\"\n",
        "if False:\n",
        "    baseline_image_features = gen_baseline(d, \"./node_webgl_render_agentic/screenshots\", clip_model[d], clip_preprocess[d])\n",
        "    print(\"baseline_image_features\", len(baseline_image_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "data_file_name = \"./model_outputs_llama8b_vision.json\"\n",
        "data_file_name_backup = \"./model_outputs_llama8b_vision_backup.json\"\n",
        "the_data = { 'data': [], 'sims': [] }\n",
        "try:\n",
        "    with open(data_file_name, 'r') as file:\n",
        "        the_data = json.load(file)\n",
        "        print(\"the_data\", len(the_data['data']))\n",
        "        \n",
        "        with open(data_file_name_backup, 'w') as file:\n",
        "            json.dump(the_data, file)\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    \n",
        "    with open(data_file_name_backup, 'r') as file:\n",
        "        the_data = json.load(file)\n",
        "        print(\"the_data\", len(the_data['data']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import uuid\n",
        "from IPython.display import clear_output\n",
        "import threading\n",
        "\n",
        "def download_in_background(\n",
        "    base_prompt,\n",
        "    the_javascript,\n",
        "    golden_model_uids,\n",
        "    data_set_iters,\n",
        "    the_output_json,\n",
        "    extra_string,\n",
        "    world_uuid\n",
        "):\n",
        "    the_output, formatted_prompt = gen_vr_content(the_javascript, base_prompt, extra_string)\n",
        "    \n",
        "    # Copy the golden UIDs to avoid mutating the original directly (if that's intended).\n",
        "    model_UIDS = golden_model_uids.copy()\n",
        "\n",
        "    for model_depth in range(model_iters):\n",
        "        for random_iter in range(num_randomized + 1):\n",
        "            this_output_iter = the_output\n",
        "            if random_iter > 0:\n",
        "                this_output_iter = modify_function_calls(this_output_iter, random_iter)\n",
        "\n",
        "            the_data_set_iter = {\"world_title\": base_prompt, \"javascript\": the_javascript, \"prompt\": formatted_prompt, \"response\": this_output_iter, 'rank': -1, \"model_depth\": model_depth, 'world_uid': world_uuid}\n",
        "            \n",
        "            next_prompt = remove_js_comments(extract_js_blocks(this_output_iter)).replace('.','.').replace('\\n\\n','\\n').replace('\\n\\n','\\n').replace('\\n\\n','\\n').replace('\\n\\n','\\n')\n",
        "            next_prompt = '\\n'.join([line.strip() for line in next_prompt.split('\\n')]).replace('\\n.', '.')\n",
        "            assert(len(next_prompt.split('\\n')) > 0)\n",
        "            next_prompt = next_prompt.split('\\n')[0].strip()\n",
        "\n",
        "            world_text = the_javascript + \"\\n\" + next_prompt\n",
        "\n",
        "            list_of_models = re.findall(r'place3dmodel\\(\"(.*?)\"', world_text)\n",
        "\n",
        "            all_used_models = []\n",
        "\n",
        "            model_UIDS = golden_model_uids.copy()\n",
        "            for modelToDownload in list_of_models:\n",
        "                if not modelToDownload in model_UIDS:\n",
        "                    try:\n",
        "                        # full_model_title = f\"{modelToDownload} in a {base_prompt}\"\n",
        "                        full_model_title = f\"{modelToDownload}\"\n",
        "                        print(\"modelToDownload\", full_model_title, model_depth)\n",
        "                        model_uid = downloadModel(full_model_title, model_depth)\n",
        "                        all_used_models.append(model_uid)\n",
        "                        model_UIDS[modelToDownload] = model_uid\n",
        "                    except Exception as e:\n",
        "                        print(\"Exception\", e)\n",
        "                        \n",
        "            for key in model_UIDS:\n",
        "                world_text = f\"\"\"load3dModel(\"{key}\", \"{model_UIDS[key]}\")\\n\"\"\" + world_text\n",
        "                \n",
        "            the_data_set_iter['model_UIDS'] = model_UIDS\n",
        "            data_set_iters.append(the_data_set_iter)\n",
        "            \n",
        "            the_output_json['options'].append(world_text)\n",
        "            print(world_text)\n",
        "            \n",
        "            with open('./node_webgl_render_agentic/world_text.json', 'w') as file:\n",
        "                json.dump(the_output_json, file)\n",
        "        time.sleep(0.01)\n",
        "    time.sleep(0.05)\n",
        "\n",
        "use_two_pass = False\n",
        "\n",
        "def generate_vr_world(base_prompt):\n",
        "\n",
        "    world_uuid = str(uuid.uuid4())\n",
        "\n",
        "    global the_data\n",
        "    random_number = random.randint(0, 10000)\n",
        "    image_folder_name = f\"./vr_screenshots/{base_prompt}_{random_number}\"\n",
        "    os.mkdir(image_folder_name)\n",
        "\n",
        "    golden_model_uids = { }\n",
        "    the_javascript = f'room(\"{base_prompt}\").topLeft(0, 0).bottomRight(15, 15);'\n",
        "    \n",
        "    pos_prompts = [f\"A gray room with correctly sized objects based on \\\"{base_prompt}\\\"\", \n",
        "                   \"a large gray room with well organized and correctly sized furnature\", \n",
        "                   f\"rich detailed 3D environment with well aligned & organized objects and large scale architecture features titled {base_prompt} (with no overlapping, disconnected, or hovering objects and no doors, windows, or pictures hoving in the middle)  Objects aren't overcrowded.\", 'attractive & well organized room with doors, pictures, and windows flush (attached to) with the walls', \n",
        "                   'symmetrical room design with appealing lines and geometry (well organized and thoughtful)', \n",
        "                   'cohesive furnature layout (master interior design)', \n",
        "                   'gorgeous room design (as if done by a master artist)', \n",
        "                   'aesthetically pleasing architecture (as if designed by a master architecht)', \n",
        "                   'attractive room design (with wonderful colors, amazing design choices, and cool objects)']\n",
        "    neg_prompts = [\"ugly room design with clashing colors and poor lighting and overcrowded objects\", \"cluttered room design with intersecting, hovering, disconnected, or overlapping objects\", \"out of proportion objects (objects with bizarre sizes)\", 'windows, doors, or pictures floating in the center of the room', 'objects too large or too crowded (objects crowding eachother)', 'bad design with poor object placements, sizes, and choices with crowded objects', 'awkward and overcrowded room design', 'overcrowded and unattractive room design', 'ugly object placement in a room', 'a room design that makes no sense']\n",
        "    pos_weights=[0.2, 0.2, 4.5, 0.2, 0.2, 0.2, 0.2, 0.2, 0.5]\n",
        "    neg_weights=[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.1, 0.1, 0.5, 0.1]\n",
        "\n",
        "    directory_path = \"./node_webgl_render_agentic/screenshots\"\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        image_folder_name_withiter = f\"{image_folder_name}/{i}\"\n",
        "        os.mkdir(image_folder_name_withiter)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        data_set_iters = []\n",
        "        the_output_json = { 'options': [], 'prompt': f\"{base_prompt}_{i}\" }\n",
        "\n",
        "        group_similarities = {}\n",
        "        images_already_seen = set()\n",
        "\n",
        "        image_proc_threads = []\n",
        "        image_sims = []\n",
        "        d = \"cuda:1\"\n",
        "        for _ in range(num_image_proc_threads):\n",
        "            image_proc_thread = threading.Thread(\n",
        "                target=find_similar_images_with_negatives_threaded,\n",
        "                args=(\n",
        "                    pos_prompts, \n",
        "                    neg_prompts, \n",
        "                    directory_path,   \n",
        "                    clip_model[d],     \n",
        "                    clip_preprocess[d], \n",
        "                    pos_weights, \n",
        "                    neg_weights, \n",
        "                    d,\n",
        "                    250,\n",
        "                    group_similarities,\n",
        "                    images_already_seen,\n",
        "                    quadro_delay\n",
        "                ),\n",
        "                daemon=True  # or False, depending on whether you want\n",
        "                            # to allow the main program to exit if this thread is still running\n",
        "            )\n",
        "            image_proc_thread.start()\n",
        "            image_proc_threads.append(image_proc_thread)\n",
        "            \n",
        "        d = \"cuda:0\"\n",
        "        for _ in range(num_image_proc_threads * 5):\n",
        "            image_proc_thread = threading.Thread(\n",
        "                target=find_similar_images_with_negatives_threaded,\n",
        "                args=(\n",
        "                    pos_prompts, \n",
        "                    neg_prompts, \n",
        "                    directory_path,   \n",
        "                    clip_model[d],     \n",
        "                    clip_preprocess[d], \n",
        "                    pos_weights, \n",
        "                    neg_weights, \n",
        "                    d,\n",
        "                    batchsize_3090,\n",
        "                    group_similarities,\n",
        "                    images_already_seen,\n",
        "                    delay_3090\n",
        "                ),\n",
        "                daemon=True  # or False, depending on whether you want\n",
        "                            # to allow the main program to exit if this thread is still running\n",
        "            )\n",
        "            image_proc_thread.start()\n",
        "            image_proc_threads.append(image_proc_thread)\n",
        "\n",
        "\n",
        "        variation_threads = []\n",
        "        for v in range(num_variations):\n",
        "            # Create a thread that executes `download_in_background`\n",
        "\n",
        "            extra_string = \"\"\n",
        "            if v == 0:\n",
        "                extra_string = \"\\nfocus on placing terrain features: windows, doors, ceiling beams, pools, gardens, balconies, stairs, & stairsteps etc...\"\n",
        "            elif v == 0:\n",
        "                extra_string = \"\\nfocus objects on top of other objects.  Books on shelves.  Plates on tables.  Papers on chairs etc...\"\n",
        "\n",
        "            if i > 7:\n",
        "                if i % 4 > 0:\n",
        "                    extra_string = \"\\nfocus on placing new objects on the ceiling (focus on light objects and large ceiling features like beams or skylights).\"\n",
        "                else:\n",
        "                    extra_string = \"\\nfocus on placing new objects on the walls (like lights, windows, doors, or archways).\"\n",
        "\n",
        "            background_thread = threading.Thread(\n",
        "                target=download_in_background,\n",
        "                args=(\n",
        "                    base_prompt,\n",
        "                    the_javascript,\n",
        "                    golden_model_uids,\n",
        "                    data_set_iters,\n",
        "                    the_output_json,\n",
        "                    extra_string,\n",
        "                    world_uuid\n",
        "                ),\n",
        "                daemon=True  # or False, depending on whether you want\n",
        "                            # to allow the main program to exit if this thread is still running\n",
        "            )\n",
        "\n",
        "            # Start the thread\n",
        "            background_thread.start()       \n",
        "            variation_threads.append(background_thread)\n",
        "            time.sleep(min(12, v * 2))\n",
        "\n",
        "\n",
        "        print(\"image_proc_thread.join()\", len(image_sims))\n",
        "        \n",
        "        for b in variation_threads:\n",
        "            b.join()\n",
        "\n",
        "        # unload_model(TABBY_BASE_URLS[0])\n",
        "        \n",
        "        for image_proc_thread in image_proc_threads:\n",
        "            image_proc_thread.join()\n",
        "\n",
        "        # --- Compute average similarity for each group ---\n",
        "        group_avg_similarities = []\n",
        "        for gid, sims in group_similarities.items():\n",
        "            avg_sim = sum(sims) / len(sims)\n",
        "            group_avg_similarities.append((gid, avg_sim))\n",
        "            \n",
        "        # unload_clip_model(\"cuda:0\")\n",
        "\n",
        "        # --- Sort groups by average similarity (descending) ---\n",
        "        group_avg_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        best_image_cat = group_avg_similarities[0][0]\n",
        "        best_image_sim = group_avg_similarities[0][1]\n",
        "        the_data['sims'].append({ 'best_image_sim': best_image_sim, 'iteration': i, 'uid': world_uuid, 'base_prompt': base_prompt })\n",
        "        print(\"best_image_sim\", best_image_sim)\n",
        "\n",
        "        if do_open_best_images:\n",
        "            try:\n",
        "                for i in range(4):\n",
        "                    image = Image.open(f\"{directory_path}/{best_image_cat}_color_{i}.png\")\n",
        "                    image.show()\n",
        "            except Exception as e:\n",
        "                print(\"Exception\", e)\n",
        "        print(\"That's all folks!\", len(group_avg_similarities), \"best:\", best_image_cat)\n",
        "\n",
        "        import shutil\n",
        "        for i in range(32):\n",
        "            try:\n",
        "                shutil.copy(f\"{directory_path}/{best_image_cat}_color_{i}.png\", image_folder_name_withiter + f\"/{best_image_cat}_color_{i}.jpg\")  \n",
        "            except Exception as e:\n",
        "                print(\"Exception\", e)\n",
        "\n",
        "        clear_screenshot_dir()\n",
        "        image_sims = group_avg_similarities\n",
        "        print(\"image_sims\", image_sims)\n",
        "        the_rank = 0\n",
        "        for item in image_sims:\n",
        "            name = int(item[0])\n",
        "            print(\"name\", name)\n",
        "            \n",
        "            if the_rank == 0:\n",
        "                print(\"name\", name)\n",
        "                next_prompt = remove_js_comments(extract_js_blocks(data_set_iters[name - 1]['response'])).replace('.','.').replace('\\n\\n','\\n').replace('\\n\\n','\\n').replace('\\n\\n','\\n').replace('\\n\\n','\\n')\n",
        "                next_prompt = '\\n'.join([line.strip() for line in next_prompt.split('\\n')]).replace('\\n.', '.')\n",
        "\n",
        "                golden_model_uids = data_set_iters[name - 1]['model_UIDS']\n",
        "                the_lines = next_prompt.split('\\n')\n",
        "                if len(the_lines) > 0:\n",
        "                    next_prompt = the_lines[0]\n",
        "                    print(next_prompt)\n",
        "                    the_javascript += \"\\n\" + next_prompt.strip()\n",
        "\n",
        "            data_set_iters[name - 1]['rank'] = the_rank\n",
        "            print(data_set_iters[name - 1], the_rank)\n",
        "            the_rank += 1\n",
        "        \n",
        "        for item in data_set_iters:\n",
        "            the_data['data'].append(item)\n",
        "        print(\"len(the_data)\", len(the_data['data']))\n",
        "\n",
        "        with open(data_file_name, 'w') as file:\n",
        "            json.dump(obj=the_data,fp=file)\n",
        "        \n",
        "    for key in golden_model_uids:\n",
        "        the_javascript = f\"\"\"load3dModel(\"{key}\", \"{golden_model_uids[key]}\")\\n\"\"\" + the_javascript\n",
        "        \n",
        "    return the_javascript\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_vr_prompts(file_path = \"./indoor_vr_prompts.txt\"):\n",
        "    vr_prompts = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            vr_prompts.append(line.strip())\n",
        "        \n",
        "    return vr_prompts\n",
        "\n",
        "vr_prompts = read_vr_prompts()\n",
        "print(\"vr_prompts\", len(vr_prompts))\n",
        "\n",
        "import random\n",
        "random.shuffle(vr_prompts)\n",
        "\n",
        "clear_screenshot_dir()\n",
        "\n",
        "num_world_tries = 1\n",
        "do_it = not do_vision_test_mode\n",
        "if do_it:\n",
        "    for prompt in vr_prompts:\n",
        "        for _ in range(num_world_tries):\n",
        "            the_final_world = generate_vr_world(prompt)\n",
        "            print(\"Final World JS\", the_final_world)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if do_vision_test_mode:\n",
        "    directory_path = \"./node_webgl_render_agentic/test_screenshots\"\n",
        "\n",
        "    total_num_variations_per_steps = len(os.listdir(directory_path))\n",
        "    num_image_proc_threads = 3\n",
        "    image_proc_threads = []\n",
        "    base_prompt = \"Lighthouse Keep's study\"\n",
        "   \n",
        "    pos_prompts = [f\"A gray room with correctly sized objects based on \\\"{base_prompt}\\\"\", \"a large gray room with well organized and correctly sized furnature\", base_prompt, 'attractive & well organized room', 'symmetrical room design', 'cohesive furnature layout', 'good room design', 'aesthetically pleasing architecture', 'attractive room design']\n",
        "    neg_prompts = [\"ugly room design\", \"cluttered room design\", \"out of proportion objects\", 'small room within a room', 'objects too large', 'bad design', 'awkward room design', 'unattractive room design', 'ugly object placement in a room', 'glitched, inverted, fragmented, compressed, or floating structures', 'Lighting Inconsistencies']\n",
        "    pos_weights=[0.4, 0.4, 0.8, 0.2, 0.2, 0.2, 0.2, 0.2, 0.5]\n",
        "    neg_weights=[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.1, 0.1, 0.5, 0.4, 0.4]\n",
        "\n",
        "    images_already_seen = set()\n",
        "    group_similarities = {}\n",
        "\n",
        "    d = \"cuda:1\"\n",
        "    for _ in range(num_image_proc_threads):\n",
        "        image_proc_thread = threading.Thread(\n",
        "            target=find_similar_images_with_negatives_threaded,\n",
        "            args=(\n",
        "                pos_prompts, \n",
        "                neg_prompts, \n",
        "                directory_path,   \n",
        "                clip_model[d],     \n",
        "                clip_preprocess[d], \n",
        "                pos_weights, \n",
        "                neg_weights, \n",
        "                d,\n",
        "                250,\n",
        "                group_similarities,\n",
        "                images_already_seen,\n",
        "                8.0\n",
        "            ),\n",
        "            daemon=True  # or False, depending on whether you want\n",
        "                        # to allow the main program to exit if this thread is still running\n",
        "        )\n",
        "        image_proc_thread.start()\n",
        "        image_proc_threads.append(image_proc_thread)\n",
        "        \n",
        "    d = \"cuda:0\"\n",
        "    for _ in range(num_image_proc_threads * 5):\n",
        "        image_proc_thread = threading.Thread(\n",
        "            target=find_similar_images_with_negatives_threaded,\n",
        "            args=(\n",
        "                pos_prompts, \n",
        "                neg_prompts, \n",
        "                directory_path,   \n",
        "                clip_model[d],     \n",
        "                clip_preprocess[d], \n",
        "                pos_weights, \n",
        "                neg_weights, \n",
        "                d,\n",
        "                50,\n",
        "                group_similarities,\n",
        "                images_already_seen,\n",
        "                0\n",
        "            ),\n",
        "            daemon=True  # or False, depending on whether you want\n",
        "                        # to allow the main program to exit if this thread is still running\n",
        "        )\n",
        "        image_proc_thread.start()\n",
        "        image_proc_threads.append(image_proc_thread)\n",
        "\n",
        "    time.sleep(60)\n",
        "\n",
        "    group_avg_similarities = []\n",
        "    for gid, sims in group_similarities.items():\n",
        "        avg_sim = sum(sims) / len(sims)\n",
        "        group_avg_similarities.append((gid, avg_sim))\n",
        "    \n",
        "    # --- Sort groups by average similarity (descending) ---\n",
        "    group_avg_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(group_avg_similarities)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./model_outputs_llama8b_vision.json', 'r') as file:\n",
        "    input_output_pairs = json.load(file)\n",
        "    print(len(input_output_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_pairs = []\n",
        "seen = set()\n",
        "\n",
        "for pair in input_output_pairs:\n",
        "    try:\n",
        "        # Convert dict into something that CAN go into a set, e.g. JSON string\n",
        "        # (which assumes everything in the dict is JSON-serializable).\n",
        "        rep = json.dumps((pair['prompt'], pair['response']), sort_keys=True)\n",
        "        \n",
        "        if rep not in seen:\n",
        "            seen.add(rep)\n",
        "            unique_pairs.append(pair)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "print(len(input_output_pairs), \"items before dedupe\")\n",
        "print(len(unique_pairs), \"items after dedupe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def format_response(in_response):\n",
        "    new_response = \"\"\n",
        "    has_first_model =  False\n",
        "    for line in in_response.split('\\n'):\n",
        "        if \"place3dmodel\" in line:\n",
        "            if has_first_model:\n",
        "                break\n",
        "            has_first_model = True\n",
        "        if \"```\" in line and has_first_model:\n",
        "            break\n",
        "\n",
        "        new_response += line +  \"\\n\"\n",
        "\n",
        "    return new_response + \"```\"\n",
        "\n",
        "\n",
        "actually_save = True\n",
        "import json\n",
        "input_output_pairs = unique_pairs\n",
        "\n",
        "import itertools\n",
        "\n",
        "longest_prompt = \"\"\n",
        "longest_ans = \"\"\n",
        "avg_prompt = 0\n",
        "avg_ans = 0\n",
        "num_counts = 0\n",
        "top_slice_data = []\n",
        "\n",
        "def generate_ab_pairs(items):\n",
        "    global longest_prompt\n",
        "    global longest_ans\n",
        "    global avg_ans\n",
        "    global avg_prompt\n",
        "    global num_counts\n",
        "\n",
        "    k = int(max(1, len(items) / 5))\n",
        "    items_sorted = sorted(items, key=lambda x: x['rank'])  # Sort by rank (ascending)\n",
        "    bottom_k_items = items_sorted[:k]                      # Get bottom k items\n",
        "    for item in bottom_k_items:\n",
        "        item['response'] = format_response(item['response'])\n",
        "        top_slice_data.append(item)\n",
        "\n",
        "        if len(item['prompt']) > len(longest_prompt):\n",
        "            longest_prompt = item['prompt']\n",
        "        if len(item['response']) > len(longest_prompt):\n",
        "            longest_ans = item['response']\n",
        "        avg_prompt += len(item['prompt'])\n",
        "        avg_ans += len(item['response'])\n",
        "        num_counts += 1\n",
        "\n",
        "    pairs = []\n",
        "    # Create all unique pairs (order doesn't matter initially)\n",
        "    if False:\n",
        "        for item1, item2 in itertools.combinations(items, 2):\n",
        "            \n",
        "            if item1['rank'] != item2['rank'] and item1['prompt'] == item2['prompt'] and item1['response'] != item2['response']:\n",
        "                if item1['rank'] < item2['rank']:\n",
        "                    pair = {'chosen': item1['response'], 'rejected': item2['response'], 'prompt': item1['prompt']}\n",
        "                else:\n",
        "                    pair = {'chosen': item2['response'], 'rejected': item1['response'], 'prompt': item1['prompt']}\n",
        "                pairs.append(pair)\n",
        "\n",
        "    return pairs\n",
        "\n",
        "by_js = { }\n",
        "for item in input_output_pairs:\n",
        "    js = item['javascript']\n",
        "    if js not in by_js:\n",
        "        by_js[js] = []\n",
        "    by_js[js].append(item)\n",
        "    \n",
        "\n",
        "the_pairs = []\n",
        "\n",
        "for key in by_js:\n",
        "    try:\n",
        "        the_pairs += generate_ab_pairs(by_js[key])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "print(\"len(the_pairs)\", len(the_pairs))\n",
        "\n",
        "import json\n",
        "\n",
        "if actually_save:\n",
        "    with open('llama8b_vision_ab_pairs.json', 'w') as file:\n",
        "        json.dump(the_pairs, file)\n",
        "    with open('llama8b_vision_top_slice.json', 'w') as file:\n",
        "        json.dump(top_slice_data, file)\n",
        "print(\"actually_save\", actually_save)\n",
        "\n",
        "print(\"top_slice_data\", len(top_slice_data))\n",
        "print(\"len(longest_prompt)\", len(longest_prompt))\n",
        "print(\"len(longest_ans)\", len(longest_ans))\n",
        "print(\"avg_prompt\", avg_prompt / num_counts)\n",
        "print(\"avg_ans\", avg_ans / num_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(longest_ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"saved_model_data.json\", \"w\") as f:\n",
        "    saved_model_data = { \"existing_searches_done\": existing_searches_done, \"list_of_all_models\": list_of_all_models, \"index_to_model_uid\": index_to_model_uid }\n",
        "    json.dump(saved_model_data, f)\n",
        "torch.save(existing_image_features.cpu().detach(), existing_image_features_filename)\n",
        "torch.save(existing_text_features.cpu().detach(), existing_text_features_filename)\n",
        "print(\"list_of_all_models\", len(list_of_all_models))\n",
        "print(\"the_data\", len(the_data['sims']))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
